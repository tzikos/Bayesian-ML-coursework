{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02477 Exercise 3: Bayesian Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import jax.numpy as jnp\n",
    "from jax import value_and_grad\n",
    "from jax import grad\n",
    "from jax import random\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snb\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "snb.set_theme(font_scale=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rev. 3.4, 3/2-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to become familiar with Bayesian linear regression and reinforce the basic concepts of Bayesian inference. \n",
    "In the exercise, you will implement several parts of Bayesian linear regression pipeline. However, remember that the purpose of the exercise is to study and understand the key equations for Bayesian linear regression as well as to learn how to use them. \n",
    "Besides being useful for practical purposes, the code also allows you to build intuition, e.g. by investigating what happens when you change different parameters etc. So think of the code as a tool for understanding the models and their key equations and not only as an end-product.\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Part 1: Visualizing the prior, likelihood and posterior\n",
    "- Part 2: Bayesian linear regression as a Linear Gaussian system\n",
    "- Part 3: Implementing the posterior distribution\n",
    "- Part 4: The posterior predictive distribution\n",
    "- Part 5: Modelling the number of Airline passengers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: The exercise contains several **discussion questions**, which are questions, where are supposed to actively experiment with the code and/or reason with the equations to arrive at the relevant conclusions. This also means that we won't provide a specific solution for this task. However, you are more than welcome to check your understanding and your conclusions with the TAs. Instead of proving the full description for every discussion question, we simply tag it with: [**Discussion question**] after the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is perhaps the most common technique in applied statistics and machine learning for modelling the relationship between set of a covariates $\\left\\lbrace \\mathbf{x}_n \\right\\rbrace_{n=1}^N$ and a response variable $\\left\\lbrace y_n \\right\\rbrace_{n=1}^N$. Rather than modelling the relationship between $\\mathbf{x}_n$ and $y_n$ directly, we often apply a transformation $\\phi$ to the input vectors $\\mathbf{x}_n$ first. That is, we often use $\\mathbf{\\phi}_n = \\mathbf{\\phi}(\\mathbf{x}_n)$ as input to regression model rather than $\\mathbf{x}_n$ directly:\n",
    "\n",
    "\\begin{align*}\n",
    "y_n = f(\\mathbf{x}_n) + e_n = \\phi(\\mathbf{x}_n)^T \\mathbf{w} + e_n = \\mathbf{\\phi}_n^T \\mathbf{w} + e_n,\n",
    "\\end{align*}\n",
    "\n",
    "where $f(\\mathbf{x_n}) \\equiv \\mathbf{\\phi}_n^T \\mathbf{w}$.\n",
    "\n",
    "\n",
    "More generally, let $\\mathbf{\\Phi} \\in \\mathbb{R}^{N \\times D}$ be a **design matrix** and let  $\\mathbf{y} \\in \\mathbb{R}^N$ be the response variables, then the linear regression model is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y}= \\mathbf{\\Phi}\\mathbf{w} + \\mathbf{e},\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^D$ is the regression weights and $\\mathbf{e} \\in \\mathbb{R}^N$ is a noise vector. In this exercise, we will only look at additive isotropic Gaussian noise models, i.e. $e_n \\sim \\mathcal{N}(0, \\sigma^2)$, but later we will study more general set-ups. Recall, the **maximum likelihood estimator** of the weights $\\mathbf{w}$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{w}}_{\\text{MLE}} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{y}\n",
    "\\end{align*}\n",
    "\n",
    "We will now turn to the Bayesian treatment. Assuming isotropic Gaussian noise and imposing a multivariate Gaussian prior on $\\mathbf{w} \\sim \\mathcal{N}\\left(\\mathbf{m}_0, \\mathbf{S}_0\\right)$ gives rise to the following joint distribution\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}, \\mathbf{w}) = p\\left(\\mathbf{y}|\\mathbf{w}\\right)p\\left(\\mathbf{w}\\right) = \\mathcal{N}\\left(\\mathbf{y}\\big|\\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}\\right)\\mathcal{N}\\left(\\mathbf{w}\\big|\\mathbf{m}_0, \\mathbf{S}_0\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\sigma^2$ is the noise variance. We will follow Bishop's notation and use the **noise precision** $\\beta = \\frac{1}{\\sigma^2}$ to parametrize the noise variance.\n",
    "\n",
    "#### The posterior distribution \n",
    "\n",
    "In contrast to the model we worked with last week, this model is an example of a **conjugate** model, meaning that the posterior distribution will also be a Gaussian distribution given by:\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S})\n",
    "\\end{align*}\n",
    "\n",
    "where the posterior covariance is given by\n",
    "\\begin{align*}\n",
    "    \\mathbf{S} &= \\left(\\mathbf{S}_0^{-1} + \\beta \\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\tag{1}\n",
    "\\end{align*}\n",
    "and the posterior mean\n",
    "\\begin{align*}\n",
    "\\mathbf{m} &= \\beta \\mathbf{S}\\mathbf{\\Phi}^T\\mathbf{y}.  \\tag{2}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **posterior distribution** $p(\\mathbf{w}|\\mathbf{y})$ quantifies our knowledge of the parameter after having observed the data $\\mathbf{y}$. Hence, we can use the posterior distribution to reason about the size of the coefficients in the linear model. For example, we can use the posterior mean as an estimator of the coefficients and the posterior variance to quantify the uncertainty. \n",
    "\n",
    "#### The posterior predictive distribution\n",
    "\n",
    "The posterior distribution also plays a crucial role when computing the **posterior predictive distribution**\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y^*|\\mathbf{y}, \\mathbf{x}^*) &= \\int p(y^*|\\mathbf{x}^*, \\mathbf{w})p(\\mathbf{w}|\\mathbf{y})\\text{d}\\mathbf{w} = \\int \\mathcal{N}(y^*|\\phi^T_* \\mathbf{w}, \\beta^{-1}) \\mathcal{N}(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}) \\text{d}\\mathbf{w}\n",
    "= \\mathcal{N}(y_*|\\phi_*^T\\mathbf{m}, \\phi_*^T\\mathbf{S} \\phi_* + \\beta^{-1}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which we will dive deeper into later in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The marginal likelihood\n",
    "\n",
    "The **marginal likelihood** or **model evidence** for this model can be computed analytically and is given by:\n",
    "\n",
    "$$p(\\mathbf{y}) = \\int p(\\mathbf{y}|\\mathbf{w})p(\\mathbf{w})d\\mathbf{w} = \\mathcal{N}(\\mathbf{y}|\\bm{0}, \\alpha^{-1}\\Phi^T \\Phi + \\beta^{-1}\\mathbf{I}).$$\n",
    "\n",
    "Note that sometimes we use the notation $p(\\mathbf{y}|\\alpha, \\beta)$ to make the dependence on the **hyperparameters** more elaborate.\n",
    "\n",
    "We will see in last parts of the exercise that the marginal likelihood can be useful for model selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we will look at the following a simple model:\n",
    "\\begin{align*}\n",
    "y_n = a + bx_n +  e_n = \\begin{bmatrix}1&x_n\\end{bmatrix}\\begin{bmatrix}a\\\\b\\end{bmatrix} + e_n = \\mathbf{\\phi}_n^T \\mathbf{w} + e_n.\n",
    "\\end{align*}\n",
    "\n",
    "That is, $\\mathbf{w} = \\begin{bmatrix}a \\\\ b \\end{bmatrix}$, where $a$ and $b$ are the intercept and slope of the line, respectively, and $\\phi(x_n) = \\begin{bmatrix}1 \\\\ x_n \\end{bmatrix}$. Furthermore, we will assume a zero-mean and isotropic Gaussian prior, i.e. $\\mathbf{m}_0 = \\mathbf{0}$, $\\mathbf{S}_0 = \\alpha^{-1}\\mathbf{I}$,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1:  Visualizing the prior, likelihood and posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will work with the following toy dataset with $N = 20$ data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "xtrain = jnp.array([1.764, 0.4, 0.979, 2.241, 1.868, -0.977,  0.95, -0.151, -0.103, 0.411, 0.144, 1.454, 0.761, 0.122,\n",
    "              0.444, 0.334, 1.494, -0.205,  0.313, -0.854])[:, None]\n",
    "ytrain = jnp.array([-0.464, 2.024, 3.191, 2.812, 6.512, -3.022, 1.99, 0.009, 2.513, 3.194, 0.935, 3.216, 0.386, -2.118,\n",
    "               0.674, 1.222, 4.481, 1.893, 0.422,  -1.209])[:, None]\n",
    "\n",
    "# plot\n",
    "def plot_toydata(ax):\n",
    "    ax.plot(xtrain, ytrain, 'k.', label='Data', markersize=12)\n",
    "    ax.set(xlabel='Input x', ylabel='Response y', xlim=(-3, 3), ylim=(-6, 6))\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plot_toydata(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this part is to understand the basic building blocks of the linear model and reinforce the concepts of prior, likelihood, and posterior. First, we will define a convenient function for evaluating and plotting distributions using densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(ax, x, y, density_fun, color=None, visibility=1, label=None, title=None, num_points = 100):\n",
    "    \n",
    "    # create grid for parameters (a,b)\n",
    "    a_array = jnp.linspace(-4, 4, num_points)\n",
    "    b_array = jnp.linspace(-4, 4, num_points)\n",
    "    A_array, B_array = jnp.meshgrid(a_array, b_array)   \n",
    "    \n",
    "    # form array with all combinations of (a,b) in our grid\n",
    "    AB = jnp.column_stack((A_array.ravel(), B_array.ravel()))\n",
    "    \n",
    "    # evaluate density for every point in the grid and reshape bac\n",
    "    Z = density_fun(x, y, A_array.ravel(), B_array.ravel())\n",
    "    Z = Z.reshape((len(a_array), len(b_array)))\n",
    "    \n",
    "    # plot contour  \n",
    "    ax.contour(a_array, b_array, jnp.exp(Z), colors=color, alpha=visibility)\n",
    "    ax.plot([-1000], [-1000], color=color, label=label)\n",
    "    ax.set(xlabel='slope', ylabel='intercept', xlim=(-4, 4), ylim=(-4, 4), title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will implement the prior, likelihood and posterior densities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 5\n",
    "beta = 3/4\n",
    "\n",
    "# log normal probability density function\n",
    "log_npdf = lambda x, m, v: -0.5*jnp.log(2*jnp.pi*v) -0.5*(x-m)**2/v \n",
    "\n",
    "# simple function for making predictions\n",
    "predict = lambda x, a, b: a + b*x\n",
    "\n",
    "def log_prior(x, y, a, b):\n",
    "    return multivariate_normal.logpdf(jnp.column_stack([a, b]), jnp.array([0, 0]), 1/alpha*jnp.identity(2))\n",
    "\n",
    "def log_likelihood(x, y, a, b):\n",
    "    return jnp.sum(log_npdf(y, predict(x, a, b), 1/beta), 0)\n",
    "\n",
    "def log_posterior(x, y, a, b):\n",
    "    return log_prior(x, y, a, b) + log_likelihood(x, y, a, b)\n",
    "\n",
    "def design_matrix(x):\n",
    "    return jnp.column_stack((jnp.ones(len(x)), x))\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,6))\n",
    "plot_distribution(axes[0], xtrain, ytrain, density_fun=log_prior, color='b', title='Prior')\n",
    "plot_distribution(axes[1], xtrain, ytrain, density_fun=log_likelihood, color='r', title='Likelihood')\n",
    "plot_distribution(axes[2], xtrain, ytrain, density_fun=log_prior, color='b', visibility=0.25, label='Prior')\n",
    "plot_distribution(axes[2], xtrain, ytrain, density_fun=log_likelihood, color='r', visibility=0.25, label='Likelihood')\n",
    "plot_distribution(axes[2], xtrain, ytrain, density_fun=log_posterior, color='g', label='Posterior', title='Posterior')\n",
    "\n",
    "##################################################################\n",
    "# Insert your solution here\n",
    "##################################################################\n",
    "##################################################################\n",
    "# end of solution\n",
    "##################################################################\n",
    "\n",
    "axes[1].legend(loc='lower right');\n",
    "axes[2].legend(loc='lower right');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: Compute the maximum likelihood solution (MLE), the MAP solution and the posterior mean for the slope and intercept. Plot all three points in the right-most panel above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: Explain the role of $\\alpha$ and $\\beta$ based on the plots above. What happens if you increase or decrease $\\alpha$ or $\\beta$?  [**Discussion question**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: What happens to the posterior mean and covariance if you let $\\beta$ approach zero (this is equivalent to the noise variance going to infinity)? Explain what you see using eq. (1) and (2) above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 1.4** Show that when we make the prior distribution infinitely wide, i.e. $\\alpha \\rightarrow 0$, then the posterior mean converges to the maximum likelihood solution.\n",
    "\n",
    "\n",
    "*Hints*: \n",
    "- Recall: $\\mathbf{S}_0 = \\alpha^{-1}\\mathbf{I}$\n",
    "- Substitute the expression for the posterior covariance in eq. (1) into the expression for the posterior mean in eq. (2) and take the limit $\\alpha \\rightarrow 0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:  Bayesian linear regression as a Linear Gaussian system\n",
    "\n",
    "The Bayesian linear regression model is a special case of a **linear Gaussian system** (see Section 3.3 in Murphy1). Linear Gaussian systems are extremely convenient because most probabilistic operations of interest are well-understood and exhibit closed-form solutions (e.g. conditioning and marginalization). They play a big role in probabilistic machine learning and throughout this course. It is therefore very important to become familiar with these equations. \n",
    "\n",
    "In the lecture for this week, we derived the posterior distribution of Bayesian linear regression using algebra and first principles, but we could also simply have used the equations for Linear Gaussian systems as we will see in the next  task.\n",
    "\n",
    "**Task 2.1**: Use the equations in Section 3.3.1 (p. 86-87) in Murphy1 to determine the mean and covariance of the posterior distribution $p(\\mathbf{w}|\\mathbf{m})$.\n",
    "\n",
    "*Hints*:\n",
    "- *Compare the prior and likelihood for Bayesian linear regression to the generic linear Gaussian system in eq. (3.33)-(3.34) in Murphy1. Next, use eq. (3.37) to derive the equations for the posterior mean and covariance*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2**: Use the equations in Section 3.3.1 (p. 86-87) in Murphy1 to determine the equation for the mean and covariance of the marginal likelihood $p(\\mathbf{y})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Implementing the posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you are given an incomplete implementation of Bayesian linear regression. Your task is to complete the implementation. See details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearRegression(object):\n",
    "    \n",
    "    def __init__(self, Phi, y, alpha=1., beta=1.):\n",
    "        \n",
    "        # store data and hyperparameters\n",
    "        self.Phi, self.y = Phi, y\n",
    "        self.N, self.D = Phi.shape\n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "        # compute posterior distribution\n",
    "        self.m, self.S = self.compute_posterior(alpha, beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        # perform sanity check of shapes/dimensions\n",
    "        self.check_dimensions()\n",
    "\n",
    "    def check_dimensions(self):\n",
    "        D = self.D\n",
    "        assert self.y.shape == (self.N, 1), f\"Wrong shape for data vector y.\\n For N = {N}, the shape of y must be ({N}, 1), but the actual shape is {self.y.shape}\"\n",
    "        assert self.m.shape == (D, 1), f\"Wrong shape for posterior mean.\\nFor D = {D}, the shape of the posterior mean must be ({D}, 1), but the actual shape is {self.m.shape}\"\n",
    "        assert self.S.shape == (D, D), f\"Wrong shape for posterior covariance.\\nFor D = {D}, the shape of the posterior mean must be ({D}, {D}), , but the actual shape is {self.S.shape}\"\n",
    "\n",
    "    def compute_posterior(self, alpha, beta):\n",
    "        \"\"\" computes the posterior N(w|m, S) and return m, S.\n",
    "            Shape of m and S must be (D, 1) and (D, D), respectively  \"\"\"\n",
    "        \n",
    "        #############################################\n",
    "        # Insert your solution here\n",
    "        #############################################\n",
    "        #############################################\n",
    "        # End of solution\n",
    "        #############################################\n",
    "        return m, S\n",
    "      \n",
    "    def generate_prior_samples(self, key, num_samples):\n",
    "        \"\"\" generate samples from the prior  \"\"\"\n",
    "        return random.multivariate_normal(key, jnp.zeros(len(self.m)), (1/self.alpha)*jnp.identity(len(self.m)), shape=(num_samples, ))\n",
    "    \n",
    "    def generate_posterior_samples(self, key, num_samples):\n",
    "        \"\"\" generate samples from the posterior  \"\"\"\n",
    "        return random.multivariate_normal(key, self.m.ravel(), self.S, shape=(num_samples, ))\n",
    "    \n",
    "    def predict_f(self, Phi):\n",
    "        \"\"\" computes posterior mean (mu_f) and variance (var_f) of f(phi(x)) for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_f and var_f must be (N,)\n",
    "            The function returns (mu_f, var_f)\n",
    "        \"\"\"\n",
    "        mu_f =  <insert code here>\n",
    "        var_f = <insert code here>\n",
    "        \n",
    "        # check dimensions before returning values\n",
    "        assert mu_f.shape == (Phi.shape[0],), \"Shape of mu_f seems wrong. Check your implementation\"\n",
    "        assert var_f.shape == (Phi.shape[0],), \"Shape of var_f seems wrong. Check your implementation\"\n",
    "        return mu_f, var_f\n",
    "        \n",
    "    def predict_y(self, Phi):\n",
    "        \"\"\" returns posterior predictive mean (mu_y) and variance (var_y) of y = f(phi(x)) + e for each row in Phi-matrix.\n",
    "            If Phi is a [N, D]-matrix, then the shapes of both mu_y and var_y must be (N,).\n",
    "            The function returns (mu_y, var_y)\n",
    "        \"\"\"\n",
    "        mu_f, var_f = self.predict_f(Phi)\n",
    "        mu_y = <insert code here>\n",
    "        var_y = <insert code here>\n",
    "\n",
    "        # check dimensions before returning values\n",
    "        assert mu_y.shape == (Phi.shape[0],), \"Shape of mu_y seems wrong. Check your implementation\"\n",
    "        assert var_y.shape == (Phi.shape[0],), \"Shape of var_y seems wrong. Check your implementation\"\n",
    "        return mu_y, var_y\n",
    "        \n",
    "    \n",
    "    def compute_marginal_likelihood(self, alpha, beta):\n",
    "        \"\"\" computes and returns log marginal likelihood p(y|alpha, beta) \"\"\"\n",
    "        inv_S0 = alpha*jnp.identity(self.D)\n",
    "        A = inv_S0 + beta*(self.Phi.T@self.Phi)\n",
    "        m = beta*jnp.linalg.solve(A, self.Phi.T)@self.y   # (eq. 3.53 in Bishop)\n",
    "        S = jnp.linalg.inv(A)                             # (eq. 3.54 in Bishop)\n",
    "        Em = beta/2*jnp.sum((self.y - self.Phi@m)**2) + alpha/2*jnp.sum(m**2)\n",
    "        return self.D/2*jnp.log(alpha) + self.N/2*jnp.log(beta) - Em - 0.5*jnp.linalg.slogdet(A)[1] - self.N/2*jnp.log(2*jnp.pi)\n",
    "         \n",
    "\n",
    "    def optimize_hyperparameters(self):\n",
    "        # optimizes hyperparameters using marginal likelihood\n",
    "        theta0 = jnp.array((jnp.log(self.alpha), jnp.log(self.beta)))\n",
    "        def negative_marginal_likelihood(theta):\n",
    "            alpha, beta = jnp.exp(theta[0]), jnp.exp(theta[1])\n",
    "            return -self.compute_marginal_likelihood(alpha, beta)\n",
    "\n",
    "        result = minimize(value_and_grad(negative_marginal_likelihood), theta0, jac=True)\n",
    "\n",
    "        # store new hyperparameters and recompute posterior\n",
    "        theta_opt = result.x\n",
    "        self.alpha, self.beta = jnp.exp(theta_opt[0]), jnp.exp(theta_opt[1])\n",
    "        self.m, self.S = self.compute_posterior(self.alpha, self.beta)\n",
    "        self.log_marginal_likelihood = self.compute_marginal_likelihood(self.alpha, self.beta)\n",
    "\n",
    "# sanity check of implementation\n",
    "model = BayesianLinearRegression(0.5*jnp.ones((2,2)), 2*jnp.ones((2, 1)), alpha=0.5, beta=0.5)       \n",
    "assert jnp.allclose(model.m, jnp.array([1, 1])), \"Something seems to be wrong with your implementation of the posterior mean. Please check your implementation.\" \n",
    "assert jnp.allclose(model.S, jnp.array([[1.5, -0.5], [-0.5, 1.5]])), \"Something seems to be wrong with your implementation of the posterior covariance. Please check your implementation.\" \n",
    "\n",
    "# fit model to toy dataset\n",
    "Phi_train = design_matrix(xtrain)\n",
    "model = BayesianLinearRegression(Phi_train, ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**: Complete the implementation of the function *compute_posterior* above using eq. (1) and (2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Task 3.2**: Report the posterior mean and a 95\\% credibility interval for the intercept and slope for $\\alpha=\\beta=1$. Compare your estimate to the plot of the data above. Do the results look reasonable compared to the plots above?\n",
    "\n",
    "*Hint: There are several ways to approach this. E.g. for a Gaussian random variable $\\mathcal{N}(\\mu, \\sigma^2)$, the interval $\\left[\\mu - 1.96\\sigma, \\mu + 1.96\\sigma\\right]$ contains approximately 95\\% of its probability mass.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Your solution goes here\n",
    "##############################################\n",
    "##############################################\n",
    "# End of solution\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4:  The posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use our toy dataset from above to study the relationship between the posterior distribution $p(\\mathbf{w}|\\mathbf{y})$ and the posterior predictive distribution $p(y^*|\\mathbf{x}^*, \\mathbf{y})$.\n",
    "After we obtained the posterior distribution $p(\\mathbf{w}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{w}| \\mathbf{m}, \\mathbf{S})$, we can also compute the distributions of $f_* = f(\\mathbf{x}_*) = \\phi(x_*)^T \\mathbf{w}$ and $y_* = f(\\mathbf{x}_*) + e$ for any new data point $\\mathbf{x}_*$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(f_*|\\mathbf{y}, \\mathbf{x}_*) &=  \\mathcal{N}(f_*|\\phi_*^T\\mathbf{m}, \\phi_*^T\\mathbf{S} \\phi_*)\\\\\n",
    "p(y_*|\\mathbf{y}, \\mathbf{x}_*) &=  \\mathcal{N}(y_*|\\phi_*^T\\mathbf{m}, \\phi_*^T\\mathbf{S} \\phi_* + \\beta^{-1}).\n",
    "\\end{align*}$$\n",
    "\n",
    "We often refer to the former as the **posterior distribution of $f_*$** and to latter as the **posterior predictive distribution for $y_*$**. Before studying the relationship, let's implement the two distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Go back up to the implementation of the class *BayesianLinearRegression* and complete the implementation of the functions *predict_f* and *predict_y*. These functions compute and return the mean and variance of $p(f_*|\\mathbf{x}_*, \\mathbf{y})$ and $p(y_*|\\mathbf{x}_*, \\mathbf{y})$, respectively, for a design matrix of new input points.\n",
    "\n",
    "*Hints: The cell below will do a simple sanity check of your implementation. Passing the sanity check does not guarantee that your implementation is correct, though*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check to help verify implementation\n",
    "dummy_model = BayesianLinearRegression(jnp.ones((2,2)), jnp.ones((2, 1)), 1, 1)\n",
    "dummy_mu_f, dummy_var_f = dummy_model.predict_f(jnp.ones((1,2)))\n",
    "dummy_mu_y, dummy_var_y = dummy_model.predict_y(jnp.ones((1,2)))\n",
    "\n",
    "assert jnp.allclose(dummy_mu_f, jnp.array([0.8])), \"Posterior mean of f_* seems wrong. Please check your implementation\"\n",
    "assert jnp.allclose(dummy_var_f, jnp.array([0.4])), \"Posterior variance of f_* seems wrong. Please check your implementation\"\n",
    "assert jnp.allclose(dummy_mu_y, jnp.array([0.8])), \"Posterior predictive mean of y_* seems wrong. Please check your implementation\"\n",
    "assert jnp.allclose(dummy_var_y, jnp.array([1.4])), \"Posterior predictive variance of y_* seems wrong. Please check your implementation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming your implementation above is correct, the cell below analyses the data set using $N = \\left\\lbrace 0, 1, 2, 5, 10, 20 \\right\\rbrace$, respectively, where the first row shows the results for $N = 0$ and so on and so forth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for visualizing predictions\n",
    "def plot_predictions(ax, x, mu, var, color='r', visibility=0.5, label=None):\n",
    "    lower, upper = mu - 1.96*jnp.sqrt(var), mu + 1.96*jnp.sqrt(var)\n",
    "    ax.plot(x, mu, color=color, label=label)\n",
    "    ax.plot(x, lower, color=color, linewidth=2, linestyle='--')\n",
    "    ax.plot(x, upper, color=color, linewidth=2, linestyle='--')\n",
    "    ax.fill_between(x.ravel(), lower.ravel(), upper.ravel(), color=color, alpha=visibility)\n",
    "    ax.plot(x, mu, '-', color=color, label=\"\", linewidth=2.5)\n",
    "    \n",
    "# fix seed\n",
    "seed = 123\n",
    "key = random.PRNGKey(seed)\n",
    "\n",
    "# fix hyperparameters\n",
    "alpha = 1.\n",
    "beta = 3/4\n",
    "\n",
    "# num posterior samples to plot\n",
    "num_samples = 30\n",
    "\n",
    "# design matrices\n",
    "xpred = jnp.linspace(-3, 3, 50)[:, None]\n",
    "Phi_train = design_matrix(xtrain)\n",
    "Phi_pred = design_matrix(xpred)\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, figsize=(30, 40))    \n",
    "for idx_n, n in enumerate([0, 1, 2, 5, 10, 20]):\n",
    "\n",
    "    key, sample_key = random.split(key)\n",
    "    \n",
    "    # compute posterior of regression weights \n",
    "    model = BayesianLinearRegression(Phi_train[:n, :], ytrain[:n, :], alpha, beta)\n",
    "    w_samples = model.generate_posterior_samples(sample_key, num_samples=num_samples)\n",
    "    \n",
    "    # split samples in a and b computes\n",
    "    a_samples, b_samples = w_samples[:, 0], w_samples[:, 1]\n",
    "    \n",
    "    # computer posterior function values for f and y\n",
    "    mu_f, var_f = model.predict_f(Phi_pred)\n",
    "    mu_y, var_y = model.predict_y(Phi_pred)\n",
    "    \n",
    "    # plot in data space\n",
    "    axes[idx_n, 0].plot(xtrain[:n, :], ytrain[:n, :], 'k.', markersize=12, label='Data')\n",
    "    plot_predictions(axes[idx_n, 0], xpred, mu_y, var_y, visibility=0.25, color='y', label='$p(y^*|\\\\mathbf{y}, x_*)$')\n",
    "    plot_predictions(axes[idx_n, 0], xpred, mu_f, var_f, visibility=0.25, color='g', label='$p(f^*|\\\\mathbf{y}, x_*)$')\n",
    "    axes[idx_n, 0].set_ylabel('N = %d' % n, fontweight='bold')    \n",
    "    for i, (ai, bi) in enumerate(zip(a_samples, b_samples)):\n",
    "        axes[idx_n, 0].plot(xpred, predict(xpred, ai, bi), 'g-', alpha=0.35)\n",
    "        \n",
    "    axes[idx_n, 0].legend(loc='upper left')\n",
    "    axes[idx_n, 0].set(xlabel='x')\n",
    "    \n",
    "    # plot prior\n",
    "    plot_distribution(axes[idx_n, 1], xtrain[:n, :], ytrain[:n, :], density_fun=log_prior, color='b')\n",
    "    \n",
    "    # plot likelihood\n",
    "    if n > 0:\n",
    "        plot_distribution(axes[idx_n, 2], xtrain[:n, :], ytrain[:n, :], density_fun=log_likelihood, color='r')\n",
    "        \n",
    "    # plot posterior\n",
    "    plot_distribution(axes[idx_n, 3], xtrain[:n, :], ytrain[:n, :], density_fun=log_posterior, color='g')\n",
    "    axes[idx_n, 3].plot(a_samples, b_samples, 'g.', markersize=10)\n",
    "        \n",
    "    \n",
    "axes[0, 0].set_title('Predictions', fontweight='bold');\n",
    "axes[0, 1].set_title('Prior $p(w)$', fontweight='bold');\n",
    "axes[0, 2].set_title('Likelihood $p(y|w)$', fontweight='bold');\n",
    "axes[0, 3].set_title('Posterior $p(w|y)$', fontweight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the first column shows the data as well as the predictive distribution with 95% intervals. Each of the green lines corresponds to the function $f(x)$ for different posterior samples for $\\mathbf{w}$. The columns 2, 3, and 4 show the prior, likelihood and posterior, respectively. Study the code and the plots above and answer the following questions:\n",
    "\n",
    "**Task 4.2**: What happens to prior, likelihood and posterior as the number of samples $N$ increases? [**Discussion question**]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.3**: What happens to the predictions if you keep $\\beta = \\frac{3}{4}$ and increase $\\alpha$, e.g. to $\\alpha = 100$ or $\\alpha = 1000$? Why? [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.4**: Set $\\alpha$ to $\\alpha = 1$ again. What happens if you decrease $\\beta$, e.g. to $\\beta = 0.1$? Why? [**Discussion question**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 4.5**: The variance of the predictive distribution $p(y_*|\\mathbf{y}, \\mathbf{x}_*)$ contains two components. Describe the two components in your own words and relate them to epistemic and aleatoric uncertainty. [**Discussion question**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.6**: As we get more and more data, the variance of the posterior distribution of $\\mathbf{w}$ goes to zero. What happens to the variance of $p(y_*|\\mathbf{y}, \\mathbf{x}_*)$, $p(y^*|\\mathbf{x}^*, \\mathbf{w})$, and $p(f_*|\\mathbf{y}, \\mathbf{x}_*)$ as $N$ increases? To answer this, fix $\\alpha = \\beta = 1$ and consider the point $x^* = 1$. Plot the following three variances: the posterior predictive variance of $y^*$, the conditional variance of $y^*$ given $\\mathbf{w}$ and posterior variance of $f^*$ as a function as size of the training set, i.e. for $N = \\left\\lbrace 0, 1, 2, .., 30 \\right\\rbrace$. Compare the plots to your findings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = 1, 1\n",
    "xstar = 1\n",
    "Ns = jnp.arange(30)\n",
    "\n",
    "#########################################################\n",
    "# Insert your solution here\n",
    "#########################################################\n",
    "#################################################3\n",
    "# End of solution\n",
    "#################################################3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.7**: Compute the prior mean and variance $f(x^*) = \\phi(x^*)^T \\bm{w} = a + b x^*$?*. Use the results to explain why the variance of both the posterior distribution of $f(x^*)$ and the variance of the posterior predictive distribution of $y(x^*)$ are smallest for $x^* = 0$ and both increase with $|x^*|$ for $N = 0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5:  Modelling the number of Airline passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply Bayesian linear regression to model the number of monthly airline passengers (in thousands) in the period from 1949-1960. The values $(x_n, y_n)$ denote the time in months since the beginning of 1949 and the number monthly number of passangers, respectively.\n",
    "\n",
    "So far, we have assumed that the hyperparameters $\\alpha$ and $\\beta$ of the model were fixed and known, but in practice, we almost always need to estimate them from data (or impose a prior on them and marginalize over them). As long as the number of hyperparameters is small, we can always rely on **cross-validation**. However, often the Bayesian framework offers a way to estimate these parameters based on the training set alone as follows.\n",
    "\n",
    "The ***evidence approximation*** suggests that we can simply choose the hyperparameters $\\alpha, \\beta$ that maximizes the marginal likelihood of the model\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\alpha^*, \\beta^* = \\arg\\max\\limits_{\\alpha, \\beta} \\ln p(\\mathbf{y}|\\alpha, \\beta),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $p(\\mathbf{y}|\\alpha, \\beta) = \\mathcal{N}(\\mathbf{y}|\\bm{0}, \\alpha^{-1}\\Phi^T \\Phi + \\beta^{-1}\\mathbf{I})$ is the marginal likelihood of the model. This is also often referred to as **maximum likelihood type II estimation**. We could also easily \"upgrade\" this to **MAP type II estimation** by imposing a prior on $p(\\alpha, \\beta)$, i.e.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\alpha^*, \\beta^* = \\arg\\max\\limits_{\\alpha, \\beta} \\ln p(\\mathbf{y}|\\alpha, \\beta)p(\\alpha, \\beta).\n",
    "\\end{align*}$$\n",
    "\n",
    "However, in this exercise we will use a uniform prior on $\\alpha$ and $\\beta$, i.e. $p(\\alpha, \\beta) \\propto 1$, in which case the two estimators will give the same results.\n",
    "\n",
    "Note that since both $\\alpha >0 $ and $\\beta > 0$ are strictly positive parameters (why?), we optimize $\\tilde{\\alpha} = \\log \\alpha$ and $\\tilde{\\beta} = \\log \\beta$ to enforce positivity on $\\alpha$ and $\\beta$.\n",
    "We rely on **JaX** for gradient-based optimization using automatic differentiation and use the standard optimizer in scipy.\n",
    "\n",
    "\n",
    "Let's load the data and plot the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./AirPassengers.csv')\n",
    "months = jnp.arange(0, len(data))[:, None]\n",
    "passengers = jnp.array(data['#Passengers'].values)[:, None]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(months, passengers, 'k.')\n",
    "ax.set_xlabel('Time in Months')\n",
    "ax.set_ylabel('Passengers (x1000)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of passengers is a strictly positive quantity, we can often get better results by modelling the logarithm of the number of passengers instead.  We also split the dataset into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform\n",
    "y = jnp.log(passengers)\n",
    "\n",
    "# split into  training and test\n",
    "N = len(y)\n",
    "Ntrain = 110\n",
    "Ntest = N - Ntrain\n",
    "\n",
    "x = jnp.arange(len(months))\n",
    "xtrain = x[:Ntrain]\n",
    "xtest = x[Ntrain:]\n",
    "\n",
    "ytrain = y[:Ntrain]\n",
    "ytest = y[Ntrain:]\n",
    "\n",
    "print('N      = %d' % N)\n",
    "print('Ntrain = %d' % Ntrain)\n",
    "print('Ntest  = %d' % Ntest)\n",
    "\n",
    "\n",
    "def plot_data(ax):\n",
    "    ax.plot(xtrain, ytrain, 'ro', label='Training data')\n",
    "    ax.plot(xtest, ytest, 'bo', label='Test data')\n",
    "    ax.set_xlabel('Time in Months')\n",
    "    ax.set_ylabel('Log (passengers (x1000))');\n",
    "    ax.legend()\n",
    "    ax.set_title('Airplane passenger dataset', fontweight='bold')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "plot_data(ax)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the quality of the fit, we use the  **expected log predictive densities (ELPD)** for the training and test sets\n",
    "\n",
    "\\begin{align}\n",
    "\\text{ELPD} = \\frac{1}{M}\\sum_{i=1}^M \\ln p(y^*_i|\\mathbf{y}, \\mathbf{x}^*_i),\n",
    "\\end{align}\n",
    "\n",
    "where $p(y^*_i|\\mathbf{y}, \\mathbf{x}^*_i)$ is the posterior predictive distribution for observation $y^*_i$ in some dataset $\\mathcal{D}_* = \\left\\lbrace \\mathbf{x}^*_i, y^*_i \\right\\rbrace_{i=1}^M$. In contrast to simply reporting the mean square errors or similar, the ELPD also takes the uncertainty into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the expected log predictive distribution as performance metric\n",
    "def compute_elpd(y, mu, s2):\n",
    "    \"\"\"\" y is a vector of observed values, and mu and s2 are the mean and variances of the posterior predictive distribution, respectively. \"\"\"\n",
    "    return jnp.mean(log_npdf(y.ravel(), mu.ravel(), s2.ravel()))\n",
    "\n",
    "# Creates design matrix Phi based on input\n",
    "def design_matrix(x):\n",
    "    return jnp.column_stack((jnp.ones(len(x)), x,   )) \n",
    "    #return jnp.column_stack((jnp.ones(len(x)), x, x**2))\n",
    "    #return jnp.column_stack((jnp.ones(len(x)), x,   x**2, jnp.cos(2*jnp.pi*x/12))) \n",
    "\n",
    "# prepare input data\n",
    "xpred = jnp.linspace(0, 143, 500)\n",
    "Phi_train = design_matrix(xtrain)\n",
    "Phi_test = design_matrix(xtest)\n",
    "Phi_pred = design_matrix(xpred)\n",
    "\n",
    "# fit model\n",
    "model = BayesianLinearRegression(Phi_train, ytrain, alpha=1, beta=1)\n",
    "#model.optimize_hyperparameters() \n",
    "logZ = model.log_marginal_likelihood\n",
    "\n",
    "# make predictions for plotting\n",
    "ypred_mu, ypred_s2 = model.predict_f(Phi_pred)\n",
    "tpred_mu, tpred_s2 = model.predict_y(Phi_pred)\n",
    "\n",
    "# make predictions for training and test set & evaluate performance metrics\n",
    "ytrain_mu, ytrain_s2 = model.predict_y(Phi_train)\n",
    "ytest_mu, ytest_s2 = model.predict_y(Phi_test)\n",
    "train_elpd = compute_elpd(ytrain, ytrain_mu, ytrain_s2)\n",
    "test_elpd = compute_elpd(ytest, ytest_mu, ytest_s2)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20,9))\n",
    "plot_data(ax)\n",
    "plot_predictions(ax, xpred, tpred_mu, tpred_s2, color='b', label='Posterior predictive', visibility=0.3)\n",
    "plot_predictions(ax, xpred, ypred_mu, ypred_s2, color='m', label='Posterior')\n",
    "ax.set_title('train ELPD=%4.3f, test ELPD=%4.3f, log marginal likelihood=%4.3f' % (train_elpd, test_elpd, logZ), fontweight='bold');\n",
    "ax.axvline(xtrain[-1], color='k', linestyle='--', alpha=0.5, label='Train/test split')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 5.1**: Run the code with $\\alpha = 10$ and $\\beta = 1$ as initial values. Comment on the quality of the fit. Note the values of the negative log likelihood for the training and test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task 5.2**: Estimate $\\alpha, \\beta$ by optimizing the marginal likelihood by uncommenting the line, which calls the function *optimize_hyperparameters()*. Report the estimates for $\\alpha$ and $\\beta$.\n",
    "\n",
    "\n",
    "\n",
    "**Task 5.3**: Experiment with different linear models but the uncommenting different lines in the *design_matrix()* function. Comment on the train/test ELPDs as well as the marginal likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.4**: Comment on uncertainty estimates for the training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.5**: Determine and visualize the posterior and the analytical posterior predictive distribution for the logarithm of the number of customers after 145 months?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For statistical models like $y_n = f(\\mathbf{x}_n) + e_n$, we typically refer to $e_n$ as noise, but another way to think about $e_n$ is that it represent the part of the signal we cannot explain by the model $f(\\mathbf{x}_n)$. \n",
    "\n",
    "**Task 5.6**: If you get hired by an airline company, and they asked you to provide a 95% credibility interval for the number of log passengers after 145, would you use the posterior distribution $p(f_*|y, x_*)$ or the posterior predictive distribution $p(y_*|y, x_*)$? and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.7**: Compute the predictive distribution for the number of passengers instead of the logarithm of the number of passengers. \n",
    "\n",
    "- Hints: You can either using sampling techniques or you use the fact that exponentiating a Gaussian random variable is a Log-Normal distribution with closed-form moments. See more here: [https://en.wikipedia.org/wiki/Log-normal_distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
